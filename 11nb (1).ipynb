{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMDB Movie Metadata:\n",
      "   Unnamed: 0  adult                     backdrop_path  \\\n",
      "0           0  False  /zfbjgQE1uSd9wiPTX4VzsLi0rGG.jpg   \n",
      "1           1  False  /tmU7GeKVybMWFButWEGl2M4GeiP.jpg   \n",
      "2           2  False  /kGzFbGhp99zva6oZODW5atUtnqi.jpg   \n",
      "3           3  False  /zb6fM1CX41D9rF9hdgclu0peUmy.jpg   \n",
      "4           4  False  /bxgTSUenZDHNFerQ1whRKplrMKF.jpg   \n",
      "\n",
      "                     genre_ids   id original_language  \\\n",
      "0           ['Drama', 'Crime']  278                en   \n",
      "1           ['Drama', 'Crime']  238                en   \n",
      "2           ['Drama', 'Crime']  240                en   \n",
      "3  ['Drama', 'History', 'War']  424                en   \n",
      "4                    ['Drama']  389                en   \n",
      "\n",
      "             original_title  \\\n",
      "0  The Shawshank Redemption   \n",
      "1             The Godfather   \n",
      "2     The Godfather Part II   \n",
      "3          Schindler's List   \n",
      "4              12 Angry Men   \n",
      "\n",
      "                                            overview  popularity  \\\n",
      "0  Imprisoned in the 1940s for the double murder ...     175.407   \n",
      "1  Spanning the years 1945 to 1955, a chronicle o...     201.059   \n",
      "2  In the continuing saga of the Corleone crime f...     102.946   \n",
      "3  The true story of how businessman Oskar Schind...     136.953   \n",
      "4  The defense and the prosecution have rested an...      74.742   \n",
      "\n",
      "                        poster_path release_date                     title  \\\n",
      "0  /9cqNxx0GxF0bflZmeSMuL5tnGzr.jpg   1994-09-23  The Shawshank Redemption   \n",
      "1  /3bhkrj58Vtu7enYsRolD1fZdja1.jpg   1972-03-14             The Godfather   \n",
      "2  /hek3koDUyRQk7FIhPXsa6mT2Zc3.jpg   1974-12-20     The Godfather Part II   \n",
      "3  /sF1U4EUQS8YHUYjNl3pMGNIQyr0.jpg   1993-12-15          Schindler's List   \n",
      "4  /ow3wq89wM8qd5X7hWKxiRfsFf9C.jpg   1957-04-10              12 Angry Men   \n",
      "\n",
      "   video  vote_average  vote_count  \n",
      "0  False         8.700       27572  \n",
      "1  False         8.689       20919  \n",
      "2  False         8.600       12620  \n",
      "3  False         8.567       16063  \n",
      "4  False         8.500        8812  \n",
      "MovieLens Ratings:\n",
      "   userId  movieId  rating            timestamp\n",
      "0       1        2     3.5  2005-04-02 23:53:47\n",
      "1       1       29     3.5  2005-04-02 23:31:16\n",
      "2       1       32     3.5  2005-04-02 23:33:39\n",
      "3       1       47     3.5  2005-04-02 23:32:07\n",
      "4       1       50     3.5  2005-04-02 23:29:40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load TMDB movie metadata (assuming 'id' is the movie identifier)\n",
    "tmdb_movies_df = pd.read_csv('tmdb_movie_metadata.csv')\n",
    "\n",
    "# Load MovieLens ratings data (assuming 'movieId' and 'userId' are the relevant columns)\n",
    "ratings_df = pd.read_csv('ratings.csv')\n",
    "\n",
    "# Preview the datasets to see their structure\n",
    "print(\"TMDB Movie Metadata:\")\n",
    "print(tmdb_movies_df.head())\n",
    "\n",
    "print(\"MovieLens Ratings:\")\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean TMDB dataset: Drop any rows with missing 'id' or 'overview' and fill missing values if necessary\n",
    "tmdb_movies_df = tmdb_movies_df.dropna(subset=['id', 'overview'])\n",
    "tmdb_movies_df['overview'] = tmdb_movies_df['overview'].fillna('')\n",
    "\n",
    "# Clean MovieLens dataset: Drop rows with missing values in the 'movieId' or 'rating'\n",
    "ratings_df = ratings_df.dropna(subset=['movieId', 'rating'])\n",
    "\n",
    "# Make sure column names match (e.g., 'id' in TMDB corresponds to 'movieId' in MovieLens)\n",
    "ratings_df = ratings_df.rename(columns={'movieId': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>adult</th>\n",
       "      <th>backdrop_path</th>\n",
       "      <th>genre_ids</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>release_date</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "      <td>2864</td>\n",
       "      <td>False</td>\n",
       "      <td>/hQ4pYsIbP22TMXOUdSfC2mjWrO0.jpg</td>\n",
       "      <td>['Comedy', 'Drama', 'Romance', 'Crime']</td>\n",
       "      <td>fi</td>\n",
       "      <td>Ariel</td>\n",
       "      <td>A Finnish man goes to the city to find a job a...</td>\n",
       "      <td>10.454</td>\n",
       "      <td>/ojDg0PGvs6R9xYFodRct2kdI6wC.jpg</td>\n",
       "      <td>1988-10-21</td>\n",
       "      <td>Ariel</td>\n",
       "      <td>False</td>\n",
       "      <td>7.100</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-02 23:46:13</td>\n",
       "      <td>443</td>\n",
       "      <td>False</td>\n",
       "      <td>/i8fiqKaQklF02VmQYhSxmwa8KOH.jpg</td>\n",
       "      <td>['Mystery', 'Romance', 'Thriller', 'Drama']</td>\n",
       "      <td>en</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>Story of a young woman who marries a fascinati...</td>\n",
       "      <td>21.327</td>\n",
       "      <td>/1qz3qUOHnVy7dL7M7G8jSErxE4b.jpg</td>\n",
       "      <td>1940-03-23</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>False</td>\n",
       "      <td>7.892</td>\n",
       "      <td>1772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-02 23:35:40</td>\n",
       "      <td>5645</td>\n",
       "      <td>False</td>\n",
       "      <td>/hzmmXx6UeYQeylioNbliFKjSbV7.jpg</td>\n",
       "      <td>['Adventure', 'Action', 'Thriller']</td>\n",
       "      <td>en</td>\n",
       "      <td>Live and Let Die</td>\n",
       "      <td>James Bond must investigate a mysterious murde...</td>\n",
       "      <td>28.271</td>\n",
       "      <td>/39qkrjqMZs6utwNmihVImC3ghas.jpg</td>\n",
       "      <td>1973-06-27</td>\n",
       "      <td>Live and Let Die</td>\n",
       "      <td>False</td>\n",
       "      <td>6.513</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-02 23:33:46</td>\n",
       "      <td>1863</td>\n",
       "      <td>False</td>\n",
       "      <td>/aCUka2PmLUIUINsiTNzOIqjS3sW.jpg</td>\n",
       "      <td>['Mystery', 'Thriller']</td>\n",
       "      <td>en</td>\n",
       "      <td>The 39 Steps</td>\n",
       "      <td>Richard Hanney has a rude awakening when a gla...</td>\n",
       "      <td>43.708</td>\n",
       "      <td>/yRnl3nTtKVTIBcLHHyXrrXPZWVS.jpg</td>\n",
       "      <td>1935-06-06</td>\n",
       "      <td>The 39 Steps</td>\n",
       "      <td>False</td>\n",
       "      <td>7.344</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-02 23:31:43</td>\n",
       "      <td>3204</td>\n",
       "      <td>False</td>\n",
       "      <td>/a9bChCqxLBixVknDXicw2Z30I8H.jpg</td>\n",
       "      <td>['Drama', 'Family']</td>\n",
       "      <td>en</td>\n",
       "      <td>A River Runs Through It</td>\n",
       "      <td>The Maclean brothers, Paul and Norman, live a ...</td>\n",
       "      <td>23.500</td>\n",
       "      <td>/aVP45oS2cBL4WtZ1kB7r8uarruB.jpg</td>\n",
       "      <td>1992-10-09</td>\n",
       "      <td>A River Runs Through It</td>\n",
       "      <td>False</td>\n",
       "      <td>7.029</td>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId   id  rating            timestamp  Unnamed: 0  adult  \\\n",
       "0       1    2     3.5  2005-04-02 23:53:47        2864  False   \n",
       "1       1  223     4.0  2005-04-02 23:46:13         443  False   \n",
       "2       1  253     4.0  2005-04-02 23:35:40        5645  False   \n",
       "3       1  260     4.0  2005-04-02 23:33:46        1863  False   \n",
       "4       1  293     4.0  2005-04-02 23:31:43        3204  False   \n",
       "\n",
       "                      backdrop_path  \\\n",
       "0  /hQ4pYsIbP22TMXOUdSfC2mjWrO0.jpg   \n",
       "1  /i8fiqKaQklF02VmQYhSxmwa8KOH.jpg   \n",
       "2  /hzmmXx6UeYQeylioNbliFKjSbV7.jpg   \n",
       "3  /aCUka2PmLUIUINsiTNzOIqjS3sW.jpg   \n",
       "4  /a9bChCqxLBixVknDXicw2Z30I8H.jpg   \n",
       "\n",
       "                                     genre_ids original_language  \\\n",
       "0      ['Comedy', 'Drama', 'Romance', 'Crime']                fi   \n",
       "1  ['Mystery', 'Romance', 'Thriller', 'Drama']                en   \n",
       "2          ['Adventure', 'Action', 'Thriller']                en   \n",
       "3                      ['Mystery', 'Thriller']                en   \n",
       "4                          ['Drama', 'Family']                en   \n",
       "\n",
       "            original_title                                           overview  \\\n",
       "0                    Ariel  A Finnish man goes to the city to find a job a...   \n",
       "1                  Rebecca  Story of a young woman who marries a fascinati...   \n",
       "2         Live and Let Die  James Bond must investigate a mysterious murde...   \n",
       "3             The 39 Steps  Richard Hanney has a rude awakening when a gla...   \n",
       "4  A River Runs Through It  The Maclean brothers, Paul and Norman, live a ...   \n",
       "\n",
       "   popularity                       poster_path release_date  \\\n",
       "0      10.454  /ojDg0PGvs6R9xYFodRct2kdI6wC.jpg   1988-10-21   \n",
       "1      21.327  /1qz3qUOHnVy7dL7M7G8jSErxE4b.jpg   1940-03-23   \n",
       "2      28.271  /39qkrjqMZs6utwNmihVImC3ghas.jpg   1973-06-27   \n",
       "3      43.708  /yRnl3nTtKVTIBcLHHyXrrXPZWVS.jpg   1935-06-06   \n",
       "4      23.500  /aVP45oS2cBL4WtZ1kB7r8uarruB.jpg   1992-10-09   \n",
       "\n",
       "                     title  video  vote_average  vote_count  \n",
       "0                    Ariel  False         7.100         339  \n",
       "1                  Rebecca  False         7.892        1772  \n",
       "2         Live and Let Die  False         6.513        2056  \n",
       "3             The 39 Steps  False         7.344         950  \n",
       "4  A River Runs Through It  False         7.029        1077  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the datasets on 'id' (from TMDB) and 'movieId' (from MovieLens)\n",
    "merged_df = pd.merge(ratings_df, tmdb_movies_df, on='id', how='inner')\n",
    "\n",
    "# Preview the merged dataset\n",
    "print(\"Merged DataFrame:\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>id</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>genre_ids</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Ariel</td>\n",
       "      <td>['Comedy', 'Drama', 'Romance', 'Crime']</td>\n",
       "      <td>A Finnish man goes to the city to find a job a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>['Mystery', 'Romance', 'Thriller', 'Drama']</td>\n",
       "      <td>Story of a young woman who marries a fascinati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Live and Let Die</td>\n",
       "      <td>['Adventure', 'Action', 'Thriller']</td>\n",
       "      <td>James Bond must investigate a mysterious murde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The 39 Steps</td>\n",
       "      <td>['Mystery', 'Thriller']</td>\n",
       "      <td>Richard Hanney has a rude awakening when a gla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A River Runs Through It</td>\n",
       "      <td>['Drama', 'Family']</td>\n",
       "      <td>The Maclean brothers, Paul and Norman, live a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId   id  rating                    title  \\\n",
       "0       1    2     3.5                    Ariel   \n",
       "1       1  223     4.0                  Rebecca   \n",
       "2       1  253     4.0         Live and Let Die   \n",
       "3       1  260     4.0             The 39 Steps   \n",
       "4       1  293     4.0  A River Runs Through It   \n",
       "\n",
       "                                     genre_ids  \\\n",
       "0      ['Comedy', 'Drama', 'Romance', 'Crime']   \n",
       "1  ['Mystery', 'Romance', 'Thriller', 'Drama']   \n",
       "2          ['Adventure', 'Action', 'Thriller']   \n",
       "3                      ['Mystery', 'Thriller']   \n",
       "4                          ['Drama', 'Family']   \n",
       "\n",
       "                                            overview  \n",
       "0  A Finnish man goes to the city to find a job a...  \n",
       "1  Story of a young woman who marries a fascinati...  \n",
       "2  James Bond must investigate a mysterious murde...  \n",
       "3  Richard Hanney has a rude awakening when a gla...  \n",
       "4  The Maclean brothers, Paul and Norman, live a ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only relevant columns: 'userId', 'id', 'rating', 'title', 'genre_ids', 'overview'\n",
    "merged_clean_df = merged_df[['userId', 'id', 'rating', 'title', 'genre_ids', 'overview']]\n",
    "\n",
    "# Preview the cleaned dataset\n",
    "merged_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.x  from https://downloads.apache.org/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.5.5'\n",
    "spark_version = 'spark-3.5.5'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@nitu_bola:57335\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Start Spark session\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMovieRecommendation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create Spark DataFrame from the merged Pandas DataFrame\u001b[39;00m\n\u001b[0;32m      8\u001b[0m merged_spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(merged_df)\n",
      "File \u001b[1;32mc:\\Users\\nitub\\anaconda31\\envs\\dev\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\nitub\\anaconda31\\envs\\dev\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\nitub\\anaconda31\\envs\\dev\\lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\nitub\\anaconda31\\envs\\dev\\lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\nitub\\anaconda31\\envs\\dev\\lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nitub\\anaconda31\\envs\\dev\\lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\nitub\\anaconda31\\envs\\dev\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@nitu_bola:57335\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"MovieRecommendation\").getOrCreate()\n",
    "\n",
    "# Create Spark DataFrame from the merged Pandas DataFrame\n",
    "merged_spark_df = spark.createDataFrame(merged_df)\n",
    "\n",
    "# Build the ALS model (Collaborative Filtering)\n",
    "als = ALS(userCol=\"userId\", itemCol=\"id\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(merged_spark_df)\n",
    "\n",
    "# Generate recommendations for all users\n",
    "recommendations = model.recommendForAllUsers(10)\n",
    "\n",
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# TF-IDF Vectorizer on 'overview' column\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(merged_df['overview'])\n",
    "\n",
    "# Compute cosine similarity between movies based on their overview\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to recommend similar movies based on movie title\n",
    "def recommend_movies(title, cosine_sim=cosine_sim):\n",
    "    idx = merged_df.index[merged_df['title'] == title].tolist()[0]  # Get the movie index\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))  # Get similarity scores for the given movie\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)  # Sort based on similarity scores\n",
    "    sim_scores = sim_scores[1:11]  # Get top 10 recommendations\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return merged_df['title'].iloc[movie_indices]  # Return the recommended movie titles\n",
    "\n",
    "# Example: Recommend movies similar to 'Toy Story'\n",
    "recommended_movies = recommend_movies('Toy Story')\n",
    "print(recommended_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
