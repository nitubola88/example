{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfQLBZt9-i12",
        "outputId": "34ef7c4c-8ecc-489b-dd8d-0617d93fce6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,378 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,771 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,681 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,236 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,000 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,534 kB]\n",
            "Fetched 21.9 MB in 3s (6,466 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.x  from https://downloads.apache.org/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.5.5'\n",
        "spark_version = 'spark-3.5.5'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "0nLjZAOP-mGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "tmdb_df = spark.read.json(\"movie_results.json\")\n",
        "tmdb_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "wPELXhfL_6sF",
        "outputId": "cca674e2-064f-4e9a-c593-860a4b4b464a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e18b53da012e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmdb_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movie_results.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtmdb_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"MoviesData\").getOrCreate()\n",
        "\n",
        "# Load the JSON file into a PySpark DataFrame\n",
        "df = spark.read.json(\"movie_results.json\")  # Replace with your JSON file path\n",
        "\n",
        "# Select only the minimal columns: movieId, name, and popularity\n",
        "df_selected = df.select(\n",
        "    col(\"id\").alias(\"movieId\"),  # Renaming id to movieId\n",
        "    col(\"title\").alias(\"name\"),  # Renaming title to name\n",
        "    col(\"popularity\")           # Keep popularity as is\n",
        ")\n",
        "\n",
        "# Show the result to verify\n",
        "df_selected.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBzQZdCRBIek",
        "outputId": "c76879b6-3054-4aa9-f0ad-5d5138d62c5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------------------+----------+\n",
            "|movieId|name                            |popularity|\n",
            "+-------+--------------------------------+----------+\n",
            "|950396 |The Gorge                       |210.407   |\n",
            "|1126166|Flight Risk                     |199.997   |\n",
            "|1064213|Anora                           |161.432   |\n",
            "|762509 |Mufasa: The Lion King           |140.813   |\n",
            "|1241982|Moana 2                         |137.318   |\n",
            "|939243 |Sonic the Hedgehog 3            |78.835    |\n",
            "|822119 |Captain America: Brave New World|66.759    |\n",
            "|823219 |Flow                            |104.294   |\n",
            "|927342 |Amaran                          |98.225    |\n",
            "|426889 |Le Clitoris                     |52.914    |\n",
            "|696506 |Mickey 17                       |85.297    |\n",
            "|926670 |Henry Danger: The Movie         |49.298    |\n",
            "|1084199|Companion                       |76.824    |\n",
            "|1160956|Panda Plan                      |43.516    |\n",
            "|912649 |Venom: The Last Dance           |65.533    |\n",
            "|539972 |Kraven the Hunter               |64.531    |\n",
            "|558449 |Gladiator II                    |63.606    |\n",
            "|1405338|Demon City                      |58.727    |\n",
            "|933260 |The Substance                   |55.553    |\n",
            "|1188104|The X-Treme Riders              |55.369    |\n",
            "+-------+--------------------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "# Load CSV files\n",
        "rating_df = spark.read.csv('ratings.csv', header=True, inferSchema=True)  # Replace with the correct path\n",
        "links_df = spark.read.csv('links.csv', header=True, inferSchema=True)    # Replace with the correct path\n",
        "\n",
        "# Step 1: Join the links_df with df_selected to add tmdbId\n",
        "df_with_tmdb = df_selected.join(links_df, 'movieId', 'inner')\n",
        "\n",
        "# Step 2: Join the resulting DataFrame with rating_df on 'movieId' to include ratings\n",
        "final_df = df_with_tmdb.join(rating_df, 'movieId', 'inner')\n",
        "\n",
        "# Step 3: Select the minimal required columns for final output\n",
        "final_df_selected = final_df.select(\n",
        "    col(\"userId\"),    # Keep userId\n",
        "    col(\"tmdbId\"),    # Keep tmdbId\n",
        "    col(\"name\"),      # Keep name (movie title)\n",
        "    col(\"popularity\"),  # Keep popularity\n",
        "    col(\"rating\")     # Keep rating\n",
        ")\n",
        "\n",
        "# Show the final result\n",
        "final_df_selected.show(truncate=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC2xzFOq-udO",
        "outputId": "547ce9cd-8035-474f-c77e-913914f5bc35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----------------------+----------+------+\n",
            "|userId|tmdbId|name                   |popularity|rating|\n",
            "+------+------+-----------------------+----------+------+\n",
            "|603   |8447  |Raiders of the Lost Ark|15.823    |5.0   |\n",
            "|385   |8447  |Raiders of the Lost Ark|15.823    |3.0   |\n",
            "|262   |8447  |Raiders of the Lost Ark|15.823    |3.0   |\n",
            "|199   |8447  |Raiders of the Lost Ark|15.823    |1.0   |\n",
            "|191   |8447  |Raiders of the Lost Ark|15.823    |5.0   |\n",
            "|160   |8447  |Raiders of the Lost Ark|15.823    |5.0   |\n",
            "|90    |8447  |Raiders of the Lost Ark|15.823    |5.0   |\n",
            "|314   |36344 |Shrek                  |14.761    |3.0   |\n",
            "|27    |36344 |Shrek                  |14.761    |4.0   |\n",
            "|608   |114   |Titanic                |10.837    |2.5   |\n",
            "|606   |114   |Titanic                |10.837    |4.0   |\n",
            "|603   |114   |Titanic                |10.837    |3.0   |\n",
            "|602   |114   |Titanic                |10.837    |2.0   |\n",
            "|599   |114   |Titanic                |10.837    |3.0   |\n",
            "|597   |114   |Titanic                |10.837    |4.0   |\n",
            "|594   |114   |Titanic                |10.837    |5.0   |\n",
            "|592   |114   |Titanic                |10.837    |3.0   |\n",
            "|590   |114   |Titanic                |10.837    |3.0   |\n",
            "|587   |114   |Titanic                |10.837    |4.0   |\n",
            "|584   |114   |Titanic                |10.837    |3.0   |\n",
            "+------+------+-----------------------+----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to make sure the rating values are in a numerical format and handle any nulls if present.\n",
        "final_df_selected = final_df_selected.dropna(subset=['rating'])  # Drop rows with missing ratings\n",
        "\n",
        "# Step 2: Split data into training and test sets\n",
        "(training_data, test_data) = final_df_selected.randomSplit([0.8, 0.2], seed=1234)\n"
      ],
      "metadata": {
        "id": "wIgH2LzpJPAo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "# Set up ALS with basic configuration\n",
        "als = ALS(userCol=\"userId\", itemCol=\"tmdbId\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
        "\n",
        "# Build a parameter grid for tuning\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [10, 20, 30]) \\\n",
        "    .addGrid(als.maxIter, [5, 10, 15]) \\\n",
        "    .addGrid(als.regParam, [0.1, 0.2, 0.3]) \\\n",
        "    .build()\n",
        "\n",
        "# Set up CrossValidator for better model evaluation\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "crossval = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# Run cross-validation and find the best model\n",
        "cv_model = crossval.fit(final_df)\n"
      ],
      "metadata": {
        "id": "VnXMathXKfaW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best model's RMSE\n",
        "best_model = cv_model.bestModel\n",
        "\n",
        "# Retrieve parameters\n",
        "rank = best_model.rank  # ALSModel stores rank as an attribute\n",
        "max_iter = best_model._java_obj.parent().getMaxIter()  # Accessing maxIter from the parent estimator\n",
        "reg_param = best_model._java_obj.parent().getRegParam()  # Accessing regParam from the parent estimator\n",
        "\n",
        "print(f\"Best Model's Parameters: rank={rank}, maxIter={max_iter}, regParam={reg_param}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx8HiWG2KiPm",
        "outputId": "9eff510d-10a8-460e-a34c-301700806b01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model's Parameters: rank=10, maxIter=15, regParam=0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZBFQvbfKmWf"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model\n",
        "predictions = best_model.transform(test_data)\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"RMSE of the best model: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5gZDK51dHUg",
        "outputId": "4b33a903-1065-4920-f8ef-9072a0ca34f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of the best model: 0.7180592624312921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMHez1ciKtLf"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}